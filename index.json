[{"content":"I\u0026rsquo;ve seen people complaining about some gigantic C/C++/Rust projects engulfing all of their memory during building from time to time. Fortunately, there are a few simple methods to alleviate the pain without sacrificing speed. By \u0026ldquo;simple,\u0026rdquo; I mean you don\u0026rsquo;t have to modify your code!\nRoot of the problem Usually, the most memory-consuming part of C/C++/Rust builds is the linking phase. To link object files, the linker must read all of them into memory. As more and more object files are linked together, the memory usage grows quickly.\nWhat\u0026rsquo;s worse, by default, DWARF embeds all debugging information into object files, which increases file sizes and thus the memory consumption. GCC Wiki mentioned that:\nIn a large C++ application compiled with -O2 and -g, the debug information accounts for 87% of the total size of the object files sent as inputs to the link step, and 84% of the total size of the output binary.\nNinja / CMake In case you haven\u0026rsquo;t heard about Ninja, it\u0026rsquo;s a build system similar to Makefile. Different from Makefile, it\u0026rsquo;s not for hand-writing but for meta build systems like CMake to generate. It\u0026rsquo;s also much faster than Makefile.\nNinja has a cool feature called pools. Each pool represents a limitation on the number of jobs. You can assign each of your rules to a pool. With this feature, you can limit the number of concurrent linkers, say to no more than four.\nIf you simply use a smaller -j number, the parallelism of compiling jobs will also be limited. Using Ninja\u0026rsquo;s pools gives you finer control.\nAs I just said, Ninja itself is not for hand-writing. Typically we invoke Ninja through CMake. A good news is that CMake provides two options on top of Ninja: JOB_POOL_COMPILE and JOB_POOL_LINK. It\u0026rsquo;s quite easy to apply them to your projects.\nMold Mold is the fastest linker in the world. Many people are already aware of that. However, what they may not know is that Mold provides an environment variable MOLD_JOBS to control the parallelism of Mold processes.\nThe primary reason for this environment variable is to minimize peak memory usage. Since mold is designed to operate with high parallelism, running multiple mold instances simultaneously may not be beneficial. If you execute N instances of mold concurrently, it could require N times the time and N times the memory. On the other hand, running them one after the other might still take N times longer, but the peak memory usage would be the same as running just a single instance.\nA single linker instance can hardly cause OOM (Out-of-Memory). And you don\u0026rsquo;t need to trade speed for this!\nCompared to Ninja\u0026rsquo;s solution, this one is faster and more widely applicable. Whatever build system and language you are using, just prepend the build command with mold -run. That\u0026rsquo;s it.\nSplit DWARF DWARF has an extension allowing you to split most debug information into separate .dwo files, and optionally leave index to them in object files to speedup debugging. By doing so, linker\u0026rsquo;s inputs are largely shrunk, saving you both time and memory.\nTo enable this feature in C/C++ builds, simply add -gsplit-dwarf to your compiler invocations. For example, in CMake you can use CMAKE_\u0026lt;LANG\u0026gt;_FLAGS or corresponding environment variables. To create the index, add -Wl,--gdb-index to your linker invocations.\nFor Rust builds, Cargo has this feature built-in already. Just set split-debuginfo to unpacked in your profile.\n","permalink":"https://blog.quarticcat.com/posts/no-more-oom/","summary":"I\u0026rsquo;ve seen people complaining about some gigantic C/C++/Rust projects engulfing all of their memory during building from time to time. Fortunately, there are a few simple methods to alleviate the pain without sacrificing speed. By \u0026ldquo;simple,\u0026rdquo; I mean you don\u0026rsquo;t have to modify your code!\nRoot of the problem Usually, the most memory-consuming part of C/C++/Rust builds is the linking phase. To link object files, the linker must read all of them into memory.","title":"No more OOM in C/C++/Rust builds"},{"content":"Built-in undo \u0026amp; redo No need to explain. They work as their name suggest. Many Zsh users remain unaware of these two widgets since they are not bound in viins keymap (which is the default for most users). I bind them to [Ctrl+Z] \u0026amp; [Ctrl+Y], respectively.\nmagic-space It behaves like normal space except that it can perform history expansion. That is to say, it turns !-1 to \u0026lt;last command\u0026gt; in your buffer. Usually bound to [Space].\npush-line Push the current buffer onto the buffer stack and clear the buffer. Next time the editor starts up, the buffer will be popped off the top of the buffer stack and loaded into the editing buffer.\nIt allows you to leave halfway to execute one other command and then resume. Usually bound to [Ctrl+Q].\npush-line-or-edit At the top-level (PS1) prompt, equivalent to push-line. At a secondary (PS2) prompt, move the entire current multiline construct into the editor buffer. The latter is equivalent to push-input followed by get-line.\nA more powerful version of push-line. Usually bound to [Ctrl+Q].\nIf you want to learn more, there are a few good places to go:\nhttps://zsh.sourceforge.io/Doc/Release/Zsh-Line-Editor.html https://github.com/zsh-users/zsh/tree/master/Functions/Zle Mine Word movement # Ref: https://github.com/marlonrichert/zsh-edit qc-word-widgets() { if [[ $WIDGET == *-shellword ]] { local words=(${(Z:n:)BUFFER}) lwords=(${(Z:n:)LBUFFER}) if [[ $WIDGET == *-backward-* ]] { local tail=$lwords[-1] local move=-${(N)LBUFFER%$tail*} } else { local head=${${words[$#lwords]#$lwords[-1]}:-$words[$#lwords+1]} local move=+${(N)RBUFFER#*$head} } } else { local subword=\u0026#39;([[:WORD:]]##~*[^[:upper:]]*[[:upper:]]*~*[[:alnum:]]*[^[:alnum:]]*)\u0026#39; local word=\u0026#34;(${subword}|[^[:WORD:][:space:]]##|[[:space:]]##)\u0026#34; if [[ $WIDGET == *-backward-* ]] { local move=-${(N)LBUFFER%%${~word}(?|)} } else { local move=+${(N)RBUFFER##(?|)${~word}} } } if [[ $WIDGET == *-kill-* ]] { (( MARK = CURSOR + move )) zle -f kill zle .kill-region } else { (( CURSOR += move )) } } for w in qc-{back,for}ward-{,kill-}{sub,shell}word; zle -N $w qc-word-widgets bindkey \u0026#39;^[[1;5D\u0026#39; qc-backward-subword # [Ctrl+Left] bindkey \u0026#39;^[[1;5C\u0026#39; qc-forward-subword # [Ctrl+Right] bindkey \u0026#39;^[[1;3D\u0026#39; qc-backward-shellword # [Alt+Left] bindkey \u0026#39;^[[1;3C\u0026#39; qc-forward-shellword # [Alt+Right] bindkey \u0026#39;^H\u0026#39; qc-backward-kill-subword # [Ctrl+Backspace] (in Konsole) bindkey \u0026#39;^W\u0026#39; qc-backward-kill-subword # [Ctrl+Backspace] (in VSCode) bindkey \u0026#39;^[[3;5~\u0026#39; qc-forward-kill-subword # [Ctrl+Delete] bindkey \u0026#39;^[^?\u0026#39; qc-backward-kill-shellword # [Alt+Backspace] bindkey \u0026#39;^[[3;3~\u0026#39; qc-forward-kill-shellword # [Alt+Delete] zsh-edit\u0026rsquo;s README explains the effect very well:\nThis is how built-in word widgets ({for,back}ward-word, {,backward-}-kill-word) move:\n# Zsh with default WORDCHARS=\u0026#39;*?_-.[]~=/\u0026amp;;!#$%^(){}\u0026lt;\u0026gt;\u0026#39; moves/deletes way too much: # \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; % ENV_VAR=value command --option-flag camelCaseWord ~/dir/*.ext # \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; # Zsh with WORDCHARS=\u0026#39;\u0026#39; is bit better, but skips punctuation clusters \u0026amp; doesn\u0026#39;t find subWords: # \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; % ENV_VAR=value command --option-flag camelCaseWord ~/dir/*.ext # \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; This is how zsh-edit subword widgets move:\n# Zsh Edit with WORDCHARS=\u0026#39;\u0026#39; # \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; % ENV_VAR=value command --option-flag camelCaseWord ~/dir/?*.ext # \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; # Zsh Edit with WORDCHARS=\u0026#39;~*?\u0026#39; # \u0026gt; \u0026gt; \u0026gt; \u0026gt; \u0026gt; % cd ~/dir/?*.ext # \u0026lt; \u0026lt; \u0026lt; \u0026lt; \u0026lt; This function also includes shell-word widgets to let you move with greater strides. They are more intuitive than Zsh\u0026rsquo;s select-word-style in some edge cases.\nAccept line Update 2024-03-12: I find that Roman Perepelitsa has made an excellent plugin called zsh-no-ps2 for this purpose. It\u0026rsquo;s more thoughtful and rigorous than my widget.\n# [Enter] Insert `\\n` when accept-line would result in a parse error or PS2 # Ref: https://github.com/romkatv/zsh4humans/blob/v5/fn/z4h-accept-line qc-accept-line() { if [[ $(functions[_qc-test]=$BUFFER 2\u0026gt;\u0026amp;1) == \u0026#39;\u0026#39; ]] { zle .accept-line } else { LBUFFER+=$\u0026#39;\\n\u0026#39; } } zle -N qc-accept-line bindkey \u0026#39;^M\u0026#39; qc-accept-line On occasion, we may unintentionally hit a key \u0026ndash; like ' \u0026ndash; before hitting enter. And that will result in a secondary prompt (PS2), which is poorly supported, and your first line will be recorded in history anyway.\nIn contrast, my widget gives you a newline instead in that scenario, so you can just delete back to correct your command. Moreover, my widget makes writing multi-line commands easier.\nTrim paste # Trim trailing whitespace from pasted text # Ref: https://unix.stackexchange.com/questions/693118 qc-trim-paste() { zle .bracketed-paste LBUFFER=${LBUFFER%%[[:space:]]#} } zle -N bracketed-paste qc-trim-paste Sometimes, we want to copy some command from the browser, say:\necho \u0026#39;hello world\u0026#39; We triple-click it to select the whole line and then copy. However, triple-clicking also selects the newline character. This could be annoying when pasting to the terminal. Have a try!\nThankfully, we have bracketed-paste handling all pasted text. We can hook pasting by overwriting this widget. The same trick is used by bracketed-paste-magic and bracketed-paste-url-magic.\nRationalize dot # Change `...` to `../..` # Ref: https://grml.org/zsh/zsh-lovers.html#_completion qc-rationalize-dot() { if [[ $LBUFFER == *.. ]] { LBUFFER+=\u0026#39;/..\u0026#39; } else { LBUFFER+=\u0026#39;.\u0026#39; } } zle -N qc-rationalize-dot bindkey \u0026#39;.\u0026#39; qc-rationalize-dot bindkey \u0026#39;^[.\u0026#39; self-insert-unmeta # [Alt+.] insert dot It\u0026rsquo;s a better alternative to traditional .../..../..... aliases. It\u0026rsquo;s more readable and supports unlimited levels.\nClear screen # [Ctrl+L] Clear screen but keep scrollback # Ref: https://superuser.com/questions/1389834 qc-clear-screen() { local prompt_height=$(echo -n ${(%%)PS1} | wc -l) local lines=$((LINES - prompt_height)) printf \u0026#34;$terminfo[cud1]%.0s\u0026#34; {1..$lines} # cursor down printf \u0026#34;$terminfo[cuu1]%.0s\u0026#34; {1..$lines} # cursor up zle .reset-prompt } zle -N qc-clear-screen bindkey \u0026#39;^L\u0026#39; qc-clear-screen Zsh has a built-in clear-screen widget bound to [Ctrl+L] by default. But it also erases your terminal emulator\u0026rsquo;s whole history in addition to the screen. My widget can keep the scrollback.\nAnd it\u0026rsquo;s more flexible. You can customize prompt position by adjusting the number of lines. For example, local lines=$((LINES/2)) moves the prompt to the middle.\nFuck # [Esc Esc] Correct previous command # Ref: https://github.com/ohmyzsh/ohmyzsh/blob/master/plugins/thefuck qc-fuck() { local fuck=$(THEFUCK_REQUIRE_CONFIRMATION=false thefuck $(fc -ln -1) 2\u0026gt;/dev/null) if [[ $fuck != \u0026#39;\u0026#39; ]] { compadd -Q $fuck } else { compadd -x \u0026#39;%F{red}-- no fucks given --%f\u0026#39; } } zle -C qc-fuck complete-word qc-fuck bindkey \u0026#39;\\e\\e\u0026#39; qc-fuck It\u0026rsquo;s a powerful replacement of oh-my-zsh\u0026rsquo;s sudo plugin. It calls thefuck to correct previous command. There are many cases that sudo plugin cannot handle, for example:\necho 1 \u0026gt; /sys/devices/system/cpu/cpufreq/boost You can\u0026rsquo;t simply prefix it with sudo as echo is not a program but a shell built-in command. Now, thefuck to the rescue. It can smartly prompt sudo sh -c \u0026quot;echo 1 \u0026gt; /sys/devices/system/cpu/cpufreq/boost\u0026quot;.\nIt\u0026rsquo;s better than using fuck as well. Because you can edit the command before executing it.\nMore Here are some key-bindings that aren\u0026rsquo;t up my street but are still useful.\nfzf fzf is a fuzzy finder. It can be integrated to many commands to provide a handy selection menu. fzf offers some completions and key-bindings under the shell folder. For Zsh, that includes:\n[Ctrl+T] - Paste the selected file path(s) into the command line [Alt+C] - cd into the selected directory [Ctrl+R] - Paste the selected command from history into the command line I also recommend you to take a look at fzf-tab, a fabulous completion plugin.\nxplr xplr is a TUI file explorer. Its document offers a bunch of hacks. I used to enjoy this key-binding:\n# [Ctrl+N] Navigate by xplr # This is not a widget since properly resetting prompt is hard # See https://github.com/romkatv/powerlevel10k/issues/72 bindkey -s \u0026#39;^N\u0026#39; \u0026#39;^Q cd -- $(xplr --print-pwd-as-result) \\n\u0026#39; I\u0026rsquo;ve spent enough time on this blog. There are more awesome projects but I\u0026rsquo;m too lazy to introduce all of them. Some are listed below.\nmouse.zsh zce.zsh mcfly atuin ","permalink":"https://blog.quarticcat.com/posts/some-useful-zsh-key-bindings/","summary":"Built-in undo \u0026amp; redo No need to explain. They work as their name suggest. Many Zsh users remain unaware of these two widgets since they are not bound in viins keymap (which is the default for most users). I bind them to [Ctrl+Z] \u0026amp; [Ctrl+Y], respectively.\nmagic-space It behaves like normal space except that it can perform history expansion. That is to say, it turns !-1 to \u0026lt;last command\u0026gt; in your buffer.","title":"Some useful Zsh key-bindings"},{"content":"Difftastic is a structural diff that understands syntax. The diff results it generates are very fancy, but its performance is poor, and it consumes a lot of memory. Recently, I boosted it by 4x while using only 23% of memory (#393, #395, #401). This post explains how I accomplished this. Hope it can bring you some inspiration.\nWhen I started to write this post, not all optimizations were reviewed and merged. But I will keep it updated.\nHow does difftastic work The official manual has a chapter describing its diff algorithm. Here\u0026rsquo;s my understanding:\nSay, we have two files. They are parsed to two syntax trees.\n------------------------- - A | - A - B | - B - C | - D - D | - E - E | - F We want to match them. To do this, we have two pointers to syntax nodes in two trees, respectively.\n------------------------- - A \u0026lt;- | - A \u0026lt;- - B | - B - C | - D - D | - E - E | - F In each step, we have some choices, and each of them represents a diff choice (an insertion, a deletion, unchanged, etc.). For example, here we can move both pointers by one node (pre-order traversal) since the nodes they point to are the same.\n- A ------------------------- - B \u0026lt;- | - B \u0026lt;- - C | - D - D | - E - E | - F Or we can move the left pointer by one node, which means a deletion.\n- - A ------------------------- - B \u0026lt;- | - A \u0026lt;- - C | - B - D | - D - E | - E | - F Or we can move the right pointer by one node, which means an insertion.\n+ - A ------------------------- - A \u0026lt;- | - B \u0026lt;- - B | - D - C | - E - D | - F - E | We keep moving pointers until finishing the traversal in both syntax trees. Then the moving path represents the diff result.\n- A - B - - C - D - E + - F Obviously, there are multiple paths. To find the best one, we abstract it as a shortest path problem: a vertex is a pointer pair, and an edge is a movement from one pointer pair to another. Edge lengths are manually defined based on our preferences. For example, it is preferable to see two identical nodes as unchanged rather than one insertion plus one deletion, therefore the length of the former is shorter.\n+-[1]-\u0026gt; (B, B) -\u0026gt; ... --+ | | (A, A) +-[9]-\u0026gt; (B, A) -\u0026gt; ... --+-\u0026gt; (EOF, EOF) | | +-[9]-\u0026gt; (A, B) -\u0026gt; ... --+ This is a simplified illustration. In the actual code, there are more kinds of edges, and the vertices contain more information.\nYou don\u0026rsquo;t have to fully understand how it works internally to start optimizing. As long as your optimized code logically equals the previous one, it should be good. It is typical for your comprehension of the code to progressively advance throughout the optimization process. Of course, having a deeper understanding helps you identify more optimization opportunities.\nBenchmarking \u0026amp; profiling To perform optimization, you should first define a baseline, i.e., find a benchmark, and then profile it to locate hot spots and choose which code area merits your attention. Fortunately, the official manual also offers a chapter that covers all we need.\nIn addition, I used hyperfine with some warm-up runs to improve the accuracy and stability of the result. Here\u0026rsquo;s a document of LLVM that introduces some tips to reduce benchmarking noise. I didn\u0026rsquo;t use all of them at that time, because I just knew this webpage recently. :)\nThere are many excellent profiling tools. Examples include flamegraph, which can be used for more than just CPU metrics, and valgrind, which is especially useful for memory profiling.\nApart from those fancy tools, you can try commenting out some code pieces or adding useless fields to structs and then check the performance difference. This will quickly give you a rough estimate of the amount of profit you will gain from optimizing them. Valueless targets are not worth your attention.\nAnother trick is that you can print something into stderr and then filter them by \u0026lt;command\u0026gt; \u0026gt;/dev/null 2\u0026gt;\u0026amp;1. Following that, you can do counting by | wc -l, or find the minimum / maximum value by | sort -n | head/tail -1. It\u0026rsquo;s sometimes more convenient than implementing the same functionality inside the program. You can use this trick to collect information like how frequently the control flow enters a code block.\nOptimizations Here comes the main dish. I will select several commits and describe what I have done.\nFree lunch Commits: 06b46e9\nThere are some free optimization approaches you can have a taste first. I call them \u0026ldquo;free\u0026rdquo; because applying them is effortless, you don\u0026rsquo;t even have to know about the code.\nLTO: In Rust, this can be enabled by a single line in Cargo.toml: lto = \u0026quot;thin\u0026quot; or \u0026quot;fat\u0026quot;. As for C++, if you are using CMake, then passing -DCMAKE_INTERPROCEDURAL_OPTIMIZATION=ON should work.\nPGO: For Rust, I recommend cargo-pgo. For C++, you can follow instructions on Clang\u0026rsquo;s doc.\nBOLT\nPolly\nMemory Allocator: The default memory allocator usually performs badly. Better alternatives include {je,tc,mi,sn}malloc and maybe more.\nSimple simplifications Commits: d48ee2d, 3b0edb4, b88625d\nAccording to my profile results, most memory was used for storing the Vertex type. Any simplification of this type will result in a huge improvement. I discovered that one of its fields implemented Copy but was wrapped by RefCell, which was overkilled. Cell was just enough. Moreover, I found that the internal third-party Stack type had a lot of fields that were useless for this problem. It was a very simple functional stack, so I just wrote a new one to replace it.\nRefactor parent stack Commits: 2c6b706, 5e5eef2, b95c3a6\nThe original parent stack structure was obscure.\nstruct Vertex\u0026lt;\u0026#39;a, \u0026#39;b\u0026gt; { // ... parents: Stack\u0026lt;EnteredDelimiter\u0026lt;\u0026#39;a\u0026gt;\u0026gt;, } enum EnteredDelimiter\u0026lt;\u0026#39;a\u0026gt; { PopEither((Stack\u0026lt;\u0026amp;\u0026#39;a Syntax\u0026lt;\u0026#39;a\u0026gt;\u0026gt;, Stack\u0026lt;\u0026amp;\u0026#39;a Syntax\u0026lt;\u0026#39;a\u0026gt;\u0026gt;)), PopBoth((\u0026amp;\u0026#39;a Syntax\u0026lt;\u0026#39;a\u0026gt;, \u0026amp;\u0026#39;a Syntax\u0026lt;\u0026#39;a\u0026gt;)), } | | |l| | | | |l| |r| | | | |l| |r| PopEither | | |----------------------| | | l r PopBoth | | |----------------------| | | l r PopBoth | | |----------------------| | | |r| | V | |l| |r| PopEither | Top |----------------------| Syntax here represented a syntax tree node. This structure was essentially two stacks that stored \u0026amp;'a Syntax\u0026lt;'a\u0026gt; with a constraint that two objects must be popped together if they were pushed together (marked as PopBoth).\nI first refactored them into this form:\nstruct Vertex\u0026lt;\u0026#39;a, \u0026#39;b\u0026gt; { // ... lhs_parents: Stack\u0026lt;EnteredDelimiter\u0026lt;\u0026#39;a\u0026gt;\u0026gt;, rhs_parents: Stack\u0026lt;EnteredDelimiter\u0026lt;\u0026#39;a\u0026gt;\u0026gt;, } enum EnteredDelimiter\u0026lt;\u0026#39;a\u0026gt; { PopEither(\u0026amp;\u0026#39;a Syntax\u0026lt;\u0026#39;a\u0026gt;), PopBoth(\u0026amp;\u0026#39;a Syntax\u0026lt;\u0026#39;a\u0026gt;), } | |PopEither(l)| |PopEither(r)| | |PopEither(l)| |PopEither(r)| | |PopEither(l)| | PopBoth(r) | | | PopBoth(l) | | PopBoth(r) | V | PopBoth(l) | |PopEither(r)| Top |PopEither(l)| |PopEither(r)| It was much more straightforward and consumed less memory. Then I noticed that each Syntax recorded its parent, we didn\u0026rsquo;t need to replicate this information in the stack (well, that was tricky as there were some edge cases). Therefore, the parent stack could be cropped to:\nstruct Vertex\u0026lt;\u0026#39;a, \u0026#39;b\u0026gt; { // ... lhs_parents: Stack\u0026lt;EnteredDelimiter\u0026gt;, rhs_parents: Stack\u0026lt;EnteredDelimiter\u0026gt;, } enum EnteredDelimiter { PopEither, PopBoth, } | |PopEither| |PopEither| | |PopEither| |PopEither| | |PopEither| | PopBoth | | | PopBoth | | PopBoth | V | PopBoth | |PopEither| Top |PopEither| |PopEither| One layer of parents represented one layer of delimiters. It\u0026rsquo;s extremely rare to see 63 layers of delimiters. Besides, the search space of the algorithm is about O(num_syntax_nodes * 2 ^ stack_depth). When there are 64 layers of delimiters, the algorithm is unlikely to finish within a reasonable time and memory limit. Thus, I boldly compressed this stack into a bitmap.\nstruct Vertex\u0026lt;\u0026#39;a, \u0026#39;b\u0026gt; { // ... lhs_parents: BitStack, rhs_parents: BitStack, } /// LSB is the stack top. One bit represents one `EnteredDelimiter`. /// /// Assume the underlying type is u8, then /// /// ```text /// new: 0b00000001 /// push x: 0b0000001x /// push y: 0b000001xy /// pop: 0b0000001x /// ``` struct BitStack(u64); #[repr(u64)] enum EnteredDelimiter { PopEither = 0, PopBoth = 1, } Here\u0026rsquo;s one thing to add: two vertices are different if they point to different Syntaxes or have different parent stacks. To distinguish between different vertices, we need to compare the whole stack. This is a high-frequency operation. Obviously, if two vertices have the same syntax nodes, then their parents must be equal. So comparing EnteredDelimiter sequence is enough. Under the bitmap representation, the whole stack can be compared in O(1) time.\nCan it be further optimized? Notice that in each movement, there\u0026rsquo;s at most one modification. Therefore, the parent stack pairs can be represented as a link to the previous Vertex + a modification:\nstruct Vertex\u0026lt;\u0026#39;a, \u0026#39;b\u0026gt; { // ... last_vertex: Option\u0026lt;\u0026amp;\u0026#39;b Vertex\u0026lt;\u0026#39;a, \u0026#39;b\u0026gt;\u0026gt;, change: StackChange, } enum StackChange { PopEitherLhs, PopEitherRhs, PopBoth, None, } This structure can save 8 bytes from Vertex since Vertex has some unfilled padding space (7 bytes) to store the enum. However, it cannot be compared in O(1) time. For example:\nLast Vertex: Last Vertex: | |PopEither| |PopEither| | |PopEither| |PopEither| |PopEither| |PopEither| | |PopEither| | PopBoth | |PopEither| |PopEither| | |PopEither| | PopBoth | | PopBoth | | PopBoth | V | PopBoth | |PopEither| | PopBoth | | PopBoth | Top | PopBoth | |PopEither| |PopEither| |PopEither| Change: PopEitherLhs Change: PopEitherRhs They have identical stacks but different last_vertex and change. Also, this structure is hard to pop. But we\u0026rsquo;re close. There\u0026rsquo;s one exception: two identical stack pairs must have the same last_vertex if the change is PopBoth. So we can just record such a vertex and the number of PopEither in both sides.\nstruct Vertex\u0026lt;\u0026#39;a, \u0026#39;b\u0026gt; { // ... pop_both_ancestor: Option\u0026lt;\u0026amp;\u0026#39;b Vertex\u0026lt;\u0026#39;a, \u0026#39;b\u0026gt;\u0026gt;, pop_lhs_cnt: u16, pop_rhs_cnt: u16, } | 3, 2, None | |PopEither| |PopEither| \u0026lt;--+ | |PopEither| |PopEither| | | |PopEither| | | | | 0, 0, Some ---------+ | | PopBoth | | PopBoth | \u0026lt;--+ | | | 1, 2, Some ---------+ | | PopBoth | | PopBoth | V |PopEither| |PopEither| Top |PopEither| This structure not only had O(1) comparison time but also saved 8 bytes per Vertex. And it relaxed the parent number limitation from 63 in total to u16::MAX consecutive PopEither, although 63 should be enough. All of its operations were just slightly more expensive than bitmaps or equally cheap. Lost efficiency could be won back by improved locality.\nTagged pointers Commits: d2f5e99, cb1c3e0\n\u0026amp;Syntax must be aligned to size_of::\u0026lt;usize\u0026gt;() as Syntax contains usize fields, which means some of its low bits are always zeros. We can use these bits to store information such as an enum tag. On x86-64 and some other 64 bits platforms, the top 16 bits haven\u0026rsquo;t been used yet. We can store information there as well. But for portability, I didn\u0026rsquo;t use those top bits.\nBy the way, I wrote a crate called enum-ptr dedicated to this trick.\nSkip visited vertices Commits: 3612d08, 9e11a22\nIn Dijkstra\u0026rsquo;s Algorithm, once a vertex is extracted from the heap, its distance will not be relaxed anymore. A vertex might be pushed into the heap multiple times if it was relaxed multiple times and the heap is not capable of the decrease-key operation (see pairing heap and Fibonacci heap). We can mark a vertex as \u0026ldquo;visited\u0026rdquo; after it is popped and skip such vertices. We can also skip visited neighbors.\nBe aware that things can change if you\u0026rsquo;re using the A* Algorithm. If you are doing a graph search rather than a tree search, which is just the case of difftastic, and your heuristic is admissible but not consistent, then visited vertices should be marked as \u0026ldquo;not visited\u0026rdquo; after being relaxed. Besides, the radix heap will be unavailable since it requires the extracted elements follow a monotonic sequence.\nMy final code ran for ~280ms in the benchmark. Replacing the radix heap with std\u0026rsquo;s binary heap results in an extra ~20ms. A heuristic must bring more speedup than that while keeping admissible. This is challenging. I\u0026rsquo;ve tried several ideas but never succeeded.\nReserve memory Commits: b0ab6c8, 8625f62, 97e883d\nTrivial.\nReuse memory Commits: 8a0c82a\nAfter 9e11a22, neighbor nodes were no longer required to be stored in vertices. They could be simply thrown away after the loop. The code logic became: create a neighbor vector, find neighbors, and return that vector. Then the profile result showed that creating vectors took a noticeable amount of time. So I created a vector outside the loop and reuse it in each round. This saved numerous memory operations.\nExploit invariants Commits: 9f1a0ab, d2f5e99, 5e5eef2\nThe original Vertex was like:\npub struct Vertex\u0026lt;\u0026#39;a, \u0026#39;b\u0026gt; { // ... pub lhs_syntax: Option\u0026lt;\u0026amp;\u0026#39;a Syntax\u0026lt;\u0026#39;a\u0026gt;\u0026gt;, pub rhs_syntax: Option\u0026lt;\u0026amp;\u0026#39;a Syntax\u0026lt;\u0026#39;a\u0026gt;\u0026gt;, lhs_parent_id: Option\u0026lt;SyntaxId\u0026gt;, rhs_parent_id: Option\u0026lt;SyntaxId\u0026gt;, } But I found that lhs/rhs_parent_id is Some only when lhs/rhs_syntax is None, respectively. Thus, they could be replaced by an enum and then compressed into a usize using the tagged pointer trick. This saved some instructions and memory footprint.\nLater, I found that during the entire shortest path finding procedure, the syntax tree was pinned. Thus, we didn\u0026rsquo;t need to obtain the unique SyntaxId at all. The pointer addresses were already unique. This further saved some instructions.\nPrune vertices Commits: 10ce859 to edc5516\nIt\u0026rsquo;s quite hard to explain this optimization in detail. It requires a deep understanding of the original implementation. I shall therefore simply introduce my ideas conceptually.\nIn brief, there were many vertices in the graph that held the following properties:\nOne can only go to another The edges between them are zero-weight. For example,\n--+ +--\u0026gt; | | --+-\u0026gt; (l0, r0) --[0]-\u0026gt; (l1, r1) --[0]-\u0026gt; (l2, r2) --+--\u0026gt; | | --+ +--\u0026gt; In this case, we can combine these vertices into one:\n--+ +--\u0026gt; | | --+-\u0026gt; (l012, r012) --+--\u0026gt; | | --+ +--\u0026gt; In the actual code, it\u0026rsquo;s more like \u0026ldquo;while there\u0026rsquo;s only one zero-weight edge, immediately move to the next vertex.\u0026rdquo;\nBy my estimation, this optimization shrank the search space by around 15%~25%, saving a huge amount of time and memory.\nManual inlining Commits: edc5516, adf6077\nC++ programmers are often taught that compilers make better decisions regarding whether functions should be inlined than they do. Because their lovely inline keyword can no longer affect inlining. However, there are a lot of factors to consider, some of which compiler doesn\u0026rsquo;t know. Inlining is not merely a matter of code size and call overhead.\nIt is preferable to inline frequently used functions rather than rarely used ones. Compiler doesn\u0026rsquo;t know which function is hot (unless PGO is used) but you know. Inlining enables more optimization opportunities. Because many optimizations cannot cross function boundaries, some of them heavily rely on inlining to bring cross-function code into their sight. Compiler cannot predict the outcome of inlining a function but you can (compile it multiple times). In Rust, inline does have effects, especially when crossing crate boundaries. Have a try: in my final code, removing #[inline(always)] from function next_vertex will increase the execution time by ~10%.\n","permalink":"https://blog.quarticcat.com/posts/optimize-difftastic/","summary":"Difftastic is a structural diff that understands syntax. The diff results it generates are very fancy, but its performance is poor, and it consumes a lot of memory. Recently, I boosted it by 4x while using only 23% of memory (#393, #395, #401). This post explains how I accomplished this. Hope it can bring you some inspiration.\nWhen I started to write this post, not all optimizations were reviewed and merged.","title":"How do I boost difftastic by 4x"},{"content":"As an emerging ISA, RISC-V learns a lot from its predecessors\u0026rsquo; mistakes and brings some very appealing designs. In my circles, RISC-V is frequently associated with the words \u0026ldquo;modern\u0026rdquo; and \u0026ldquo;elegant\u0026rdquo;. Its vector extension (RVV) is often given equivalent praise, even though nearly no one has used a real-world RVV machine (including me) or even programmed in RVV. After experimenting with RVV for a while, I feel that it is not as good as many people claimed.\nHow is RVV designed In contrast to SIMD architectures, RVV has variable-length vector registers. That means different chips (hardware threads, or harts, to be precise) can have different vector register lengths while sharing the same instruction set. To accomplish this, the software must get and set the length parameter with some instructions at runtime. RVV operations distinguish only between vector-vector and vector-scalar, signed and unsigned, but not element lengths. As a result, the element length is a dynamic parameter as well. Furthermore, RVV enables us to use only a portion of a vector register or to combine multiple vector registers, which necessitates the usage of a dynamic parameter. The type of a vector register is mainly governed by the factors listed below.\nVLEN: a constant, represents the length of a vector register on this chip vl: a Control and Status Register, or CSR, controls the number of elements used in operations, making it easier to handle the tail elements of an array vtype: a CSR, includes vill: represents whether the vtype configuration is ill-formed or not vma / vta: controls the operation behavior of those masked-off elements and tail elements vsew: controls the length of a single element, represented by SEW = 8 | 16 | 32 | 64 vlmul: controls how many registers are used in an operation, represented by LMUL = 1/8 | 1/4 | 1/2 | 1 | 2 | 4 | 8 We use vset{i}vl{i} instructions to set both vl and vtype.\nI need to elaborate on the parameter LMUL. When LMUL = 1/2, for example, we only use half of the registers. When LMUL = 8, we combine 8 contiguous registers into one, resulting in 32 / 8 = 4 accessible registers. Since there are only 5 bits for a register index in all RVV instructions, we don\u0026rsquo;t receive more registers when LMUL \u0026lt; 1.\nThere are a few other parameters. However, they will not be discussed in this blog, so I will not list them.\nMore details can be found in the RVV Spec. I\u0026rsquo;ll end my introduction here.\nAnnoyances of RVV C intrinsics In RVV C intrinsics, vl, vma, vta are specified at function invocations, whereas vsew, vlmul are hard-coded into types. Compilers are responsible to insert vset{i}vl{i} instructions for you. We now have the following horrible table (source: RVV Intrinsic RFC).\nData Types Encode SEW and LMUL into data types. We enforce the constraint LMUL ≥ SEW/ELEN in the implementation. There are the following data types for ELEN = 64.\nTypes LMUL = 1 LMUL = 2 LMUL = 4 LMUL = 8 LMUL = 1/2 LMUL = 1/4 LMUL = 1/8 int64_t vint64m1_t vint64m2_t vint64m4_t vint64m8_t N/A N/A N/A uint64_t vuint64m1_t vuint64m2_t vuint64m4_t vuint64m8_t N/A N/A N/A int32_t vint32m1_t vint32m2_t vint32m4_t vint32m8_t vint32mf2_t N/A N/A uint32_t vuint32m1_t vuint32m2_t vuint32m4_t vuint32m8_t vuint32mf2_t N/A N/A int16_t vint16m1_t vint16m2_t vint16m4_t vint16m8_t vint16mf2_t vint16mf4_t N/A uint16_t vuint16m1_t vuint16m2_t vuint16m4_t vuint16m8_t vuint16mf2_t vuint16mf4_t N/A int8_t vint8m1_t vint8m2_t vint8m4_t vint8m8_t vint8mf2_t vint8mf4_t vint8mf8_t uint8_t vuint8m1_t vuint8m2_t vuint8m4_t vuint8m8_t vuint8mf2_t vuint8mf4_t vuint8mf8_t vfloat64 vfloat64m1_t vfloat64m2_t vfloat64m4_t vfloat64m8_t N/A N/A N/A vfloat32 vfloat32m1_t vfloat32m2_t vfloat32m4_t vfloat32m8_t vfloat32mf2_t N/A N/A vfloat16 vfloat16m1_t vfloat16m2_t vfloat16m4_t vfloat16m8_t vfloat16mf2_t vfloat16mf4_t N/A There are the following data types for ELEN = 32.\nTypes LMUL = 1 LMUL = 2 LMUL = 4 LMUL = 8 LMUL = 1/2 LMUL = 1/4 LMUL = 1/8 int32_t vint32m1_t vint32m2_t vint32m4_t vint32m8_t N/A N/A N/A uint32_t vuint32m1_t vuint32m2_t vuint32m4_t vuint32m8_t N/A N/A N/A int16_t vint16m1_t vint16m2_t vint16m4_t vint16m8_t vint16mf2_t N/A N/A uint16_t vuint16m1_t vuint16m2_t vuint16m4_t vuint16m8_t vuint16mf2_t N/A N/A int8_t vint8m1_t vint8m2_t vint8m4_t vint8m8_t vint8mf2_t vint8mf4_t N/A uint8_t vuint8m1_t vuint8m2_t vuint8m4_t vuint8m8_t vuint8mf2_t vuint8mf4_t N/A vfloat32 vfloat32m1_t vfloat32m2_t vfloat32m4_t vfloat32m8_t N/A N/A N/A vfloat16 vfloat16m1_t vfloat16m2_t vfloat16m4_t vfloat16m8_t vfloat16mf2_t N/A N/A Mask Types Encode the ratio of SEW/LMUL into the mask types. There are the following mask types.\nn = SEW/LMUL\nTypes n = 1 n = 2 n = 4 n = 8 n = 16 n = 32 n = 64 bool vbool1_t vbool2_t vbool4_t vbool8_t vbool16_t vbool32_t vbool64_t There are a lot of N/A here, which makes it a little difficult to generate code with C macros. This is because the RVV specification has a loose VLEN restriction, requiring just that it can contain at least one largest element (i.e. VLEN \u0026gt;= ELEN). As a result, these N/A types may not be available on some chips (for example, an RV64V chip with VLEN = 64 cannot support SEW = 64, LMUL = 1/8). These types don\u0026rsquo;t seem to matter much, though, because the LMUL \u0026lt; 1 case seems to be uncommon, and is usually used in widening instructions or narrowing instructions, which do not use those N/A types.\nThanks to LMUL, the amount of intrinsic types is huge, making the size of the header file and docs megabytes large. The good news is that RVV inrtinsics provide overloaded functions. But the names of these functions are essentially mapped to assembly instructions. There are a lot of functions that could be overloaded together while they aren\u0026rsquo;t. When you try to wrap them, you still need to do a lot of extra work, as stated in this issue.\nThese are just some small annoyances during my experience of RVV. I won\u0026rsquo;t use them to criticize RVV. The major problem is that these intrinsic types are all dynamically sized types (or sizeless types, or unsized types) due to RVV\u0026rsquo;s variable-length nature. And I\u0026rsquo;m afraid that DSTs are poorly supported in all languages, not just C.\nThe ecosystem has not prepared for DSTs First of all, the C language standard actually has a DST, i.e., the variable-length array. When a VLA is constructed, the current stack frame will be dynamically extended. It\u0026rsquo;s like alloca with some extra information such as type and lifetime. The implementation of RVV intrinsic types in Clang is very similar to VLA.\nHowever, while being supported as a compiler extension by GCC and Clang, VLA is not part of the C++ standard. C++ standard does not have any DST. As for Rust, although it does have DSTs like dyn Trait and [T], they can only be held indirectly using references or pointers. Dynamically extending stack frames is not possible with Rust at all. To properly support RVV, a number of issues must be taken into account, and maybe many changes must be made in these two languages. Consider these: How do you store RVV variables as static variables? How do you put them in a struct? How do you pass them as arguments to a function and return them from a function? None of these actions can be done on VLA variables. They are so fundamental and natural to any other SIMD type, but they pose a significant challenge to RVV.\nCurrently, the vast majority of existing C++ code are written against statically sized types. We rely on the static sizes in so many places without awareness. Have you ever think of that a sizeof in some constant evaluation context may break? What\u0026rsquo;s worse, DST is colored. If a struct contains a DST, then it is a DST, too. The suffering and sorrow spreads along the dependency chain.\nSo what is the status quo? If I recall properly, LLVM/Clang only allows RVV types to be used as local variables, arguments, and return values. Other than these, none of the aforementioned uses are allowed. While GCC\u0026rsquo;s support for RVV has stuck in an intermediate state for a long time (9/29/2022 update: GCC has supported RVV v1.0). As RVV is not the first vector architecture implementation, we can investigate the language support status for its predecessor, ARM SVE, to see how far we can go. Well, the support is, not less constrained than RVV.\nIs Rust better? Rust apparently possesses more language facilities for DSTs. It has a Sized trait used everywhere, implicitly or explicitly. And it permits structs with DST fields as long as they are the final ones (which is weird since Rust doesn\u0026rsquo;t guarantee the memory layout). So I believe that the process of Rust embracing RVV will be smoother. However, as was already said, Rust is unable to dynamically extend stack frames, making it impossible to even put a DST variable on the stack.\nActually, I seriously doubt that a language exists that supports DSTs well and can build zero-cost abstractions on top of them. Lack of ecosystem support suggests that RVV will be less consistent and composable in terms of language level.\nMaybe I\u0026rsquo;ve taken the problem too seriously, because scenarios that uses SIMD/Vector are quite specific. Even though RVV lacks so much abilities, you probably won\u0026rsquo;t get affected. But those SIMD library authors will. Their code design might automatically reject RVV. Putting aside the distinctions between SIMD and vectors, there is a more pressing issue: RVV types cannot be wrapped in a struct. In addition to SIMD libraries, GCC and Clang\u0026rsquo;s vector extension is also hard to support RVV as it requires you to specify the sizes at compile-time. That indicates that there isn\u0026rsquo;t a lot of SIMD-accelerated code can support RVV cheaply. By the way, Rust\u0026rsquo;s std::simd simply gives up supporting RVV for now.\nOnly Google\u0026rsquo;s highway pronounces support RVV, as far as I\u0026rsquo;m aware. However, some of its modules, such as vqsort, don\u0026rsquo;t. It is common for other sorting networks to employ transposes, but vqsort\u0026rsquo;s sorting network uses a number of permutations to avoid transposing, making it extremely challenging to convert to RVV because it is length-agnostic. It seems that nsimd tried to support RVV but stopped a long time ago.\nChoice of intrinsic types is not clear The SIMD type to employ is typically obvious. Numerous SIMD libraries, such as C++\u0026rsquo;s experimental \u0026lt;simd\u0026gt;, can choose an underlying SIMD type for you automatically. For instance, on x86 platforms, the fallback order is commonly AVX512 -\u0026gt; AVX2 -\u0026gt; SSE2, despite the fact that the time required to switch between licenses and the degree of downclocking of AVX512 and AVX2 differ amongst microarchitectures. And since you simply need to take into account the element type, it is also evident for ARM SVE. However, things become considerably more confusing in RVV.\nRecall that RVV has a LMUL parameter. Larger LMUL values are expected to increase speed at the expense of the number of available registers, which suggests a higher likelihood of spilling. You might need to tune LMUL to get a higher efficiency. I initially believed it to be a special procedure that only RVV possesses. But after a while, I noticed how similar it is to the loop unrolling problem.\nTo some extent, compilers can decide how to unroll loops for you. Then what about LMUL? Can compilers choose a proper value for you? I don\u0026rsquo;t know. But I think this is not as easy. Because LMUL is not an opt-in feature. You cannot pretend it always equals to one. Widening and narrowing instructions (e.g., convert u32 to u64 or the reverse) will change LMUL. Taken that into consideration, the optimizing process could be more complex than loop unrolling. It reminds me of how RISC-V\u0026rsquo;s genius design bring problems to linker implementations.\nIf the compiler is unable to select an appropriate LMUL for you, then you have to tune LMUL manually. Another issue now: can SIMD libraries offer a unified, cross-platform API over it? That is challenging, in my opinion, and I haven\u0026rsquo;t come across any related design.\nGiven the resemblance between selecting LMUL values and unrolling loops, I have to wonder if LMUL is really essential. Will the speedup warrant the additional complexity it adds? We don\u0026rsquo;t know because RVV hardware is currently scarce.\nPossible higher context switch cost The context size issue plagues older vector processors (according to some articles, though, I\u0026rsquo;m not familiar with that period of history). Their vector registers are typically made to be long in order to achieve a high speedup. This method will undoubtedly bloat the context size. As a result, operating systems must spend additional time and resources on register saving during context switching.\nThe RISC-V Reader, a popular resource for RISC-V newcomers, proudly claims that RVV can avoid this problem, because RVV has a dedicated instruction vsetdcfg that can enable / disable registers by need, so that we can only pay for what we use. Sounds promising, doesn\u0026rsquo;t it? However, The RISC-V Reader is very out-dated. The instruction vsetdcfg has already been deprecated in the current RVV spec. RVV now only has a very coarse-grained mechanism that records whether any vector register is modified or not. If the vendor chooses a long-length implementation, I believe RVV will also experience the context size issue. This problem is unlikely to bother you though. Super long vectors are usually designed for HPC, of which the resource is usually dedicated to one single program at a time.\nCan RVV emulate SIMD? Some blogs and talks say that RVV can, at worst, emulate SIMD. THIS IS NOT TRUE.\nThink of that, how can we use a stuff with less information (register sizes only known at run-time) to emulate a stuff with more information (register sizes known at compile-time)? To expose unified APIs to users, the only feasible way is to erase the extra information from SIMD types. And this is what highway does.\nBut can\u0026rsquo;t we set the desired size at run-time to match with SIMD? One might ask. No, you can\u0026rsquo;t. You must first deal with the DST issue, as I elaborated. And after that, you have to deal with the big variety of VLEN. Recall that the RVV Spec only stipulates VLEN \u0026gt;= ELEN. So in some processors your vl settings might fail. What if we use LMUL to concat registers? Well, then you plunge into the type choosing problem.\nA way out is to use Zvl* extensions, which specifies the minimum vector register length. Theoretically, you can use feature flags (e.g., pre-defined macros like __AVX2__ in C++) to pick different implementations for different VLEN at compile-time or simply reject those platforms that don\u0026rsquo;t have sufficient length. Of course, once you do this, you lose the portability advantage of RVV. And even you use only a part of the register, the context size won\u0026rsquo;t be smaller. And as far as I know, such flags haven\u0026rsquo;t been implemented in LLVM/Clang yet.\nThat\u0026rsquo;s not the end. Despite all the challenges, there are some situations where SIMD emulation is still impossible (or too expansive), for instance, permutations. RVV does have some permutation instructions like slideup, slidedown, gather, scatter, and compress. And they are very helpful. You can also see some of them in AVX512. But SIMD\u0026rsquo;s general permutation instructions can do more than that; they accept a lookup table to rearrange elements within a register, which is not possible for RVV. As the length is agnostic, the lookup table size is unknown. Simdjson utilizes permutations to classify characters. Vqsort utilizes permutations to implement its sorting network. Neither of them can be cheaply emulated by RVV.\n","permalink":"https://blog.quarticcat.com/posts/rvv-may-not-be-as-good-as-you-think/","summary":"As an emerging ISA, RISC-V learns a lot from its predecessors\u0026rsquo; mistakes and brings some very appealing designs. In my circles, RISC-V is frequently associated with the words \u0026ldquo;modern\u0026rdquo; and \u0026ldquo;elegant\u0026rdquo;. Its vector extension (RVV) is often given equivalent praise, even though nearly no one has used a real-world RVV machine (including me) or even programmed in RVV. After experimenting with RVV for a while, I feel that it is not as good as many people claimed.","title":"RVV may not be as good as you think"},{"content":"As we all know that C++\u0026rsquo;s name lookup has always been extremely counter-intuitive. The infamous argument-dependent lookup (ADL), for example, often leads to unexpected behavior and, worse yet, is often difficult to troubleshoot. This can happen when you define a function in your current namespace, but when you call it the compiler selects another function with the same name thousands of miles away, even though you did not using that foreign function in the current namespace. Here are two typical examples.\nnamespace fuckadl { struct Fuck {}; void foo(Fuck) { puts(\u0026#34;not mine\u0026#34;); } } // namespace fuckadl // case 1 void foo(fuckadl::Fuck) { puts(\u0026#34;mine\u0026#34;); } // // case 2 // template\u0026lt;class T\u0026gt; // void foo(T) { puts(\u0026#34;mine\u0026#34;); } int main() { foo(fuckadl::Fuck{}); } In case 1, you will get a compilation error, which complains about ambiguity. In case 2, which is worse, you won\u0026rsquo;t even receive a warning. The compiler will simply select fuckadl::foo for you. If you are a beginner of C++, it is likely to take you a whole day to debug this problem.\nThis design directly breaks the encapsulation of namespaces, while countless header-only libraries are using namespaces to hide internal symbols from caller sites (after all, C++\u0026rsquo;s module mechanism has been delayed for so long), and you don\u0026rsquo;t know when you\u0026rsquo;ll collide with someone else\u0026rsquo;s function names.\nI recently discovered another nasty design of C++\u0026rsquo;s name lookup. Here it is, C++ Standard Draft N3337 10.2 Member name lookup [class.member.lookup]:\nMember name lookup determines the meaning of a name (id-expression) in a class scope (3.3.7). Name lookup can result in an ambiguity, in which case the program is ill-formed. For an id-expression, name lookup begins in the class scope of this; for a qualified-id, name lookup begins in the scope of the nested-name-specifier. Name lookup takes place before access control (3.4, Clause 11). The last sentence is the main point. It means that access specifiers like public and private are invisible when performing the name lookup. I\u0026rsquo;ll give a very counter-intuitive case (modified from this question):\nstruct Base { operator bool() { return true; } }; struct Derived: private Base { operator int() { return 1; } }; int main() { std::cout \u0026lt;\u0026lt; Derived{}; } Note the use of private inheritance here. To put it informally, the Derived class actually looks like this from the callers\u0026rsquo; perspective:\nstruct Derived { public: operator int() { return 1; } private: operator bool() { return true; } }; However, the compiler will emit an ambiguity error for this code! Because the compiler doesn\u0026rsquo;t see access specifiers at this point, both operator bool() and operator int() are valid candidates, and that\u0026rsquo;s why we have ambiguity.\nSince C++ does not have zero-sized types, before the advent of [[no_unique_address]] in C++20, one common way to introduce zero-sized members was to use private / protected inheritance, utilizing the empty base optimization to achieve zero size. This is not rare in template libraries. For example, Boost\u0026rsquo;s compressed_pair is implemented in this way.\nThanks to this bizarre design, we have to consider whether the names of all members of the base class will conflict with the derived class when using this technique, even if the inheritance is private. This is so outrageous that it could definitely be considered an abstraction leak.\nWait, you say you don\u0026rsquo;t use private inheritance at all? Well, it can still haunt you. Here\u0026rsquo;s a simpler example:\nstruct Base1 { public: int x = 1; }; struct Base2 { private: int x = 1; }; struct Derived: Base1, Base2 {}; int main() { Derived{}.x; } The compiler will emit an ambiguity error for this code as well for the same reason. When looking for .x, the compiler doesn\u0026rsquo;t see access specifiers. It finds that both Base1 and Base2 have an x member, which is ambiguous.\nThis example is a bit more outrageous than the last one. Imagine that, if Base2 come from outside, you won\u0026rsquo;t even see such a member name in its API document. And most likely the name of this member name is not guaranteed. After all, it\u0026rsquo;s implementation details. Yet this member will get in the way of your name lookup! Similar to ADL, this design makes private encapsulate nothing.\nOf course, also similar to ADL, there are approaches to get around this. Below is one possible solution. But the existence of this problem is really annoying.\nstruct Derived: Base1, Base2 { using Base1::x; }; Next is the most outrageous thing: ambiguity errors can trigger substitution failures, so they can be used in SFINAE or Concept. That means, by utilizing the code logic above, we can detect externally whether a class has some private / protected members.\nLet me demonstrate it. Suppose we are going to detect member x, no matter whether it is public or not, then we can construct a class A with a member x and a class B that inherits both A and the class to be detected, and then produce a substitution failure by accessing the x member of B. The code is as follows:\nstruct A { int x; }; template\u0026lt;class T\u0026gt; struct B: A, T {}; template\u0026lt;class T\u0026gt; concept TestX = !requires(B\u0026lt;T\u0026gt; b) { b.x; }; Lastly, we write two simple test cases.\nstruct HasX { private: int x; }; struct HasY { private: int y; }; static_assert(TestX\u0026lt;HasX\u0026gt;); // success static_assert(TestX\u0026lt;HasY\u0026gt;); // error ","permalink":"https://blog.quarticcat.com/posts/one-more-nasty-design-of-cpp-name-lookup/","summary":"As we all know that C++\u0026rsquo;s name lookup has always been extremely counter-intuitive. The infamous argument-dependent lookup (ADL), for example, often leads to unexpected behavior and, worse yet, is often difficult to troubleshoot. This can happen when you define a function in your current namespace, but when you call it the compiler selects another function with the same name thousands of miles away, even though you did not using that foreign function in the current namespace.","title":"One more nasty design of C++'s name lookup"},{"content":"The readability of C++\u0026rsquo;s types is terrible, and most beginner tutorials don\u0026rsquo;t go into detail about how to read them. They at most discuss the difference between top-level const and low-level const. Many of my friends have asked me about this, and I\u0026rsquo;ve talked about it numerous times. So I thought, why not write a blog post on it?\nA common misconception Before talking about type reading in detail, I\u0026rsquo;d like to address a common misconception.\nQ: What\u0026rsquo;s the type of a in int a[5]?\nA: int[5], and will decay to int* when appropriate.\nQ: What\u0026rsquo;s the type of a in int a[5][6]?\nA: int[5][6], and will decay to int(*)[6], the pointer to int[6], when appropriate.\nBecause of the existence of decay rules, many C++ beginners consider array types and pointer types the same. But they are different.\nSimilarly, function types will also decay to pointer types. For instance, int(int, int) will decay to int(*)(int, int).\nSolving equations You might have seen some people saying that reading C++\u0026rsquo;s types is just solving equations. Let me explain this statement.\nLeaving aside CVR (const, volatile, reference), the most basic declarations in C++ can be divided into two parts: the type names written on the leftmost are the type specifiers, and the rest are the declarators. These parts are inherited from C. They are the nastiest component of C++ types. These two names are not intuitive, I like to refer them as \u0026ldquo;return types\u0026rdquo; and \u0026ldquo;expressions\u0026rdquo;, as will be explained later. Let\u0026rsquo;s first start with a few examples.\nDeclaration Type Specifier Declarator int a int a int* a int *a int a[5] int a[5] int* a[5] int *a[5] int (*a)[5] int (*a)[5] After separating these two parts, it is now easy to understand how the types of as above are determined: using a as the form of the declarator, then the type of the return value is just the type specifier. The so-called equation solving is just such a process:\nFrom int *a[5] we have that the type of *a[5] is int. That is, dereferencing a[5] returns a int, which means the type of a[5] is int*. That is, indexing a returns a int*, which means the type of a is an array of 5 int*。 Another example:\nFrom int (*a)[5] we have that the type of (*a)[5] is int. That is, indexing (*a) returns a int, which means the type of (*a) is int[5]. That is, dereferencing a returns a int[5], which means the type of a is a pointer to int[5]. You can see that the operator precedence here is exactly the same as in expressions, and the form is basically the same as well.\nOne more example:\nFrom int (*a)(int, int) we have that the type of (*a)(int, int) is int. That is, calling (*a) in the form of (int, int) returns a int, which means the type of (*a) is a function that has two int parameters and returns a int, i.e., int(int, int). That is, dereferencing a returns int(int, int), which means the type of a is a pointer to int(int, int). A more complex example:\nFrom int (*(Foo::*a[5])(int, int))(int) we have that the type of (*(Foo::*a[5])(int, int))(int) is int. So the type of (*(Foo::*a[5])(int, int)) is int(int). So the type of (Foo::*a[5])(int, int) is a pointer to int(int), i.e., int(*)(int). So the type of (Foo::*a[5]) is a function that has two int parameters and returns a int(*)(int). So the type of a[5] is a member pointer to that function. So the type of a is an array of 5 that member pointers. In short, you can consider the declarator as an expression, and the type of this expression is the type specifier. Using this rule, you can infer the type backward.\nOf course, writing complex types in the way shown above is not recommended. That declaration is only for illustration. In real world code, it might be written as the following:\nstruct Foo { auto bar(int, int) -\u0026gt; int(*)(int); }; decltype(\u0026amp;Foo::bar) a[5]; Before C++11, you can use typedef to decompose that declaration to improve readability.\nstruct Foo { typedef int (*bar_t)(int); bar_t bar(int, int); }; typedef Foo::bar_t (Foo::*foo_bar_t)(int, int); foo_bar_t a[5]; Such type aliases are very common in C.\nBut if the type comes from demangling, it will be the hideous one-line looking.\nMulti-variable declarations After you can distinguish between type specifiers and declarators, it\u0026rsquo;s easy to understand the rules of C++ multi-variable declarations. In multi-variable declarations, the comma-separated declarators (the expressions) share one type specifier (the return type). For example:\nint *a, b; The type of a is int* while the type of b is int, since * is a part of the declarator. If you wish to define two pointers, you should write int *a, *b instead.\nMore complex examples follow the same rules. Since I can\u0026rsquo;t think of any point in doing this, I won\u0026rsquo;t give more examples.\nconst and volatile const and volatile take the same position in declarations, and they can be used together. Here for simplicity, only const will be used in the following examples. All const below can be (syntactically) legally replaced by volatile, const volatile, and volatile const. The latter two are semantically equal.\nconst usually has a clear meaning, as in const int a and int (*a)(const std::string\u0026amp;). Confusions of const often involve pointers. For pointers, const needs to express two meanings:\nThe pointer itself is const, i.e., it cannot point to other values, like char *const a. The pointee it points to is const, i.e., you cannot modify the pointee through the pointer, like const char* a. When there are nested pointers, we also have to consider the const property of those inner pointers, so the types become complicated. With the previous knowledge, we can divide consts into consts on type specifiers and consts on declarators. Look at the following type:\nconst int * const * * const a; The leftmost const is on the type specifier, and the rest consts are on the declarators.\nconsts on type specifiers are required to be placed aside type specifiers. Both const T and T const are legal and semantically equal. Thus, the above example can be written as:\nint const * const * * const a; They both mean that\nThe type of * const * * const a is const int (cannot modify ***a). To distinguish that which const specifies which part, you only need to take care that when you encounter a const during the equation solving.\nThe type of * * const a is a const pointer to const int (cannot modify **a). The type of * const a is a non-const pointer to the previous pointer (can modify *a). The type of a is a const pointer to the previous pointer (cannot modify a). The famous C++ book C++ Primer divides consts into top-level consts and low-level consts. The term top-level const refers to the const that specifies the pointer itself, i.e., the innermost one; the term low-level const refers to the rest.\n\u0026amp; and \u0026amp;\u0026amp; References in C++ is very special. Since C++ standard specifies that references don\u0026rsquo;t necessarily occupy memory space, you can\u0026rsquo;t write arrays of references (int\u0026amp; a[5]) and pointers to references (int\u0026amp;* a) and many other reference related types. I think it\u0026rsquo;s a huge design failure. It adds a lot of obstacles out of nowhere.\nAnyway, except for function types (and types that contains function types) where \u0026amp; and \u0026amp;\u0026amp; can appear in the argument type and return value type, in other cases \u0026amp; and \u0026amp;\u0026amp; can only appear at the top level of the type. The meaning of \u0026ldquo;top-level\u0026rdquo; here is the same as top-level const in the previous section.\nDirectly nesting \u0026amp; and \u0026amp;\u0026amp;, like int\u0026amp; \u0026amp;\u0026amp; a, is not allowed. But indirectly nesting them is allowed. For example:\nusing ref1 = int\u0026amp;\u0026amp;; using ref2 = ref1\u0026amp;\u0026amp;; using ref3 = ref2\u0026amp;; In this case, reference collapsing will occur. The rule of reference collapsing is simple: if there are all \u0026amp;\u0026amp;, then final type will be \u0026amp;\u0026amp;; if there is one \u0026amp;, then the final type will be \u0026amp;. So the ref2 above is actually int\u0026amp;\u0026amp;, while the ref3 is int\u0026amp;.\nAnother usage of \u0026amp;\u0026amp; is in templates. If T is a template parameter, then T\u0026amp;\u0026amp; (without const and volatile) is a universal reference. It has the following rules:\nIf the argument passed to T\u0026amp;\u0026amp; has type Foo\u0026amp;, then T is inferred to Foo\u0026amp;, then T\u0026amp;\u0026amp; becomes Foo\u0026amp; after reference collapsing. If the argument passed to T\u0026amp;\u0026amp; has type Foo\u0026amp;\u0026amp;, then T is inferred to Foo, then T\u0026amp;\u0026amp; becomes Foo\u0026amp;\u0026amp; after reference collapsing. Note that it\u0026rsquo;s important that T is inferred to what type, because you might want to use T somewhere inside the template.\nMember functions In C++, sometimes we can see code like this:\nstruct Foo { int a(int, int) const; }; struct Bar { int a(int, int) \u0026amp;\u0026amp;; }; This is easy to understand. Each non-static member function in C++ has an implicitly defined this pointer that points to the instance. The const and \u0026amp;\u0026amp; written after it are used to specify instances.\nOn some other languages like Rust and Python, the passed instance is written explicitly as the first parameter, whereas in C++ it is implicit (before C++23). So you have to add type specifiers elsewhere. The following examples loosely reveal how those type specifiers work.\nstruct Foo { // void a(const Foo\u0026amp; this_, int) { // const Foo* this = \u0026amp;this_; // } void a(int) const\u0026amp;; // void a(Foo\u0026amp;\u0026amp; this_, int) { // Foo* this = \u0026amp;this_; // } void a(int) \u0026amp;\u0026amp;; // void a(Foo\u0026amp; this_, int) { // Foo* this = \u0026amp;this_; // } void a(int) \u0026amp;; // foo.a(1) -\u0026gt; Foo::a(foo, 1) }; struct Bar { // void a(const Bar\u0026amp; this_, int) { // const Bar* this = \u0026amp;this_; // } // // or just: // // void a(const Bar* this, int); void a(int) const; // void a(Bar\u0026amp; this_, int) { // Bar* this = \u0026amp;this_; // } // // or just: // // void a(Bar* this, int); void a(int); // bar.a(1) -\u0026gt; Bar::a(bar, 1) // // or: // // bar.a(1) -\u0026gt; Bar::a(\u0026amp;bar, 1) }; Member functions with references cannot be overloaded with member functions without references. You either choose the set demonstrated by Foo or the set demonstrated by Bar. Some other possible overloads, such as those with volatile, are omitted.\ntypedef and using I have shown examples of typedef and using in previous sections. The syntax of typedef is exactly the same as defining a variable. The only one difference in syntax between typedef and using is that using put the type name in the left. In the following example, both foo_t and bar_t have the same type of func_ptr.\nint (*func_ptr)(int); typedef int (*foo_t)(int); using bar_t = int(*)(int); Compared to typedef, one advantage of using is that it can use templates. E.g.,\ntemplate\u0026lt;typename T\u0026gt; using foo_t = std::vector\u0026lt;T\u0026gt;; foo_t\u0026lt;int\u0026gt; foo; Before C++11, you can implement this through wrapping typedef by a struct or a class. E.g.,\ntemplate\u0026lt;typename T\u0026gt; struct Bar { typedef std::vector\u0026lt;T\u0026gt; type; }; Bar\u0026lt;int\u0026gt;::type bar; This post only discusses types, so other semantics of using will be skipped.\nMore The components of the C++ declaration statement are quite complex, not just the type specifiers and declarators. But the rest of them are much more human-friendly, and many of them are not part of types. For those who interested in learning more, please read cppreference.\nTypes could have been clear and easy to read, it\u0026rsquo;s just that C and C++\u0026rsquo;s design is terrible. Now that you\u0026rsquo;ve read this article, congratulations on solving a problem that doesn\u0026rsquo;t even exist in other languages.\n","permalink":"https://blog.quarticcat.com/posts/read-cpp-types/","summary":"The readability of C++\u0026rsquo;s types is terrible, and most beginner tutorials don\u0026rsquo;t go into detail about how to read them. They at most discuss the difference between top-level const and low-level const. Many of my friends have asked me about this, and I\u0026rsquo;ve talked about it numerous times. So I thought, why not write a blog post on it?\nA common misconception Before talking about type reading in detail, I\u0026rsquo;d like to address a common misconception.","title":"How to read C++ types"}]